{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from cclddg.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLOOB Conditioned Latent Denoising Diffusion GAN\n",
    "\n",
    "> My code and utilities for training CCLDDGs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very much a work in progress. Stay tuned for better info soon :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment I'd suggest cloning this and adding it to your path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is all this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main thing this code does is define a UNet architecture and an accompanying Discriminator architecture that can take in an image (or a latent representation of one) along with conditioning information (what timestep we're looking at, a CLOOB embedding of an image or caption) and a latent variable `z` used to turn the unet into a more GAN-like multimodal generator thingee. \n",
    "\n",
    "Coming soon, demos of this as\n",
    "- A standard diffusion model\n",
    "- A standard latent diffusion model\n",
    "- A standard Defusion Denoising GAN\n",
    "- A latent Defusion Denoising GAN\n",
    "- CLOOB-Conditioned Latent Defusion Denoising GAN\n",
    "- Training a text-to-image model with no text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for now here's a sum\n",
    "3+5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the training script\n",
    "\n",
    "The train script is written to run OUTSIDE this directory (aka NOT in cclddg). It also assumes the locations of various dependancies and model files. To set it up, in a notebook run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/johnowhitaker/cclddg                               &>> install.log\n",
    "# !git clone https://github.com/CompVis/latent-diffusion                           &>> install.log\n",
    "# !git clone https://github.com/CompVis/taming-transformers                        &>> install.log\n",
    "# !pip install -e ./taming-transformers                                            &>> install.log\n",
    "# !git clone --recursive https://github.com/crowsonkb/cloob-training               &>> install.log\n",
    "# !git clone https://github.com/openai/CLIP/                                       &>> install.log\n",
    "# !pip install CLIP/.                                                              &>> install.log\n",
    "# !pip install --upgrade webdataset ipywidgets                                     &>> install.log\n",
    "# !pip install datasets omegaconf einops wandb pytorch_lightning                   &>> install.log\n",
    "# !wget https://ommer-lab.com/files/latent-diffusion/kl-f8.zip                     &>> install.log\n",
    "# !unzip -q kl-f8.zip "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then if you wish to use W&B for logging, run `wandb login` in a terminal.\n",
    "\n",
    "Then copy the script from the cclddg folder downloaded as part of the command above to your local dir:\n",
    "\n",
    "`cp cclddg/train_cclddg.py train.py`\n",
    "\n",
    "And run your training like so:\n",
    "\n",
    "`python train.py --z_dim 16 --n_channels_unet 64 --batch_size 64 --n_batches 200 --lr_gen 0.0001 --lr_disc 0.0001 --log_images_every 50 --save_models_every 500 --n_steps 8 --dataset celebA --wandb_project cclddg_faces`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
