---

title: My Title


keywords: fastai
sidebar: home_sidebar

summary: "API details."
description: "API details."
nb_path: "00_core.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: 00_core.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Building-Blocks">Building Blocks<a class="anchor-link" href="#Building-Blocks"> </a></h2><p>This section defines the different building blocks we'll use to build the core unet and discriminator architectures.</p>
<p>First up: the activation function:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>By default this all uses the <a href="/cclddg/core.html#Swish"><code>Swish</code></a> activation function: $x \cdot \sigma(x)$</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Swish" class="doc_header"><code>class</code> <code>Swish</code><a href="https://github.com/johnowhitaker/cclddg/tree/master/cclddg/core.py#L13" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Swish</code>() :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>.. note::
    As per the example above, an <code>__init__()</code> call to the parent class
    must be made before assignment on the child.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This is what it looks like:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next, we want a way to create embeddings from various conditioning information.</p>
<p>Explain time embedding</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="TimeEmbedding" class="doc_header"><code>class</code> <code>TimeEmbedding</code><a href="https://github.com/johnowhitaker/cclddg/tree/master/cclddg/core.py#L20" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>TimeEmbedding</code>(<strong><code>n_channels</code></strong>:<code>int</code>) :: <code>Module</code></p>
</blockquote>
<h3 id="Embeddings-for-$t$">Embeddings for $t$<a class="anchor-link" href="#Embeddings-for-$t$"> </a></h3>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>more text</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="CLOOBEmbedding" class="doc_header"><code>class</code> <code>CLOOBEmbedding</code><a href="https://github.com/johnowhitaker/cclddg/tree/master/cclddg/core.py#L63" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>CLOOBEmbedding</code>(<strong><code>n_channels</code></strong>:<code>int</code>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>.. note::
    As per the example above, an <code>__init__()</code> call to the parent class
    must be made before assignment on the child.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ZEmbedding" class="doc_header"><code>class</code> <code>ZEmbedding</code><a href="https://github.com/johnowhitaker/cclddg/tree/master/cclddg/core.py#L78" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ZEmbedding</code>(<strong><code>z_dim</code></strong>:<code>int</code>, <strong><code>n_channels</code></strong>:<code>int</code>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>.. note::
    As per the example above, an <code>__init__()</code> call to the parent class
    must be made before assignment on the child.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And the rest of the building blocks:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ResidualBlock" class="doc_header"><code>class</code> <code>ResidualBlock</code><a href="https://github.com/johnowhitaker/cclddg/tree/master/cclddg/core.py#L97" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ResidualBlock</code>(<strong><code>in_channels</code></strong>:<code>int</code>, <strong><code>out_channels</code></strong>:<code>int</code>, <strong><code>time_channels</code></strong>:<code>int</code>, <strong><code>n_groups</code></strong>:<code>int</code>=<em><code>32</code></em>) :: <code>Module</code></p>
</blockquote>
<h3 id="Residual-block">Residual block<a class="anchor-link" href="#Residual-block"> </a></h3><p>A residual block has two convolution layers with group normalization.
Each resolution is processed with two residual blocks.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="AttentionBlock" class="doc_header"><code>class</code> <code>AttentionBlock</code><a href="https://github.com/johnowhitaker/cclddg/tree/master/cclddg/core.py#L148" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>AttentionBlock</code>(<strong><code>n_channels</code></strong>:<code>int</code>, <strong><code>n_heads</code></strong>:<code>int</code>=<em><code>1</code></em>, <strong><code>d_k</code></strong>:<code>int</code>=<em><code>None</code></em>, <strong><code>n_groups</code></strong>:<code>int</code>=<em><code>32</code></em>) :: <code>Module</code></p>
</blockquote>
<h3 id="Attention-block">Attention block<a class="anchor-link" href="#Attention-block"> </a></h3><p>This is similar to <a href="../../transformers/mha.html">transformer multi-head attention</a>.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="DownBlock" class="doc_header"><code>class</code> <code>DownBlock</code><a href="https://github.com/johnowhitaker/cclddg/tree/master/cclddg/core.py#L215" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>DownBlock</code>(<strong><code>in_channels</code></strong>:<code>int</code>, <strong><code>out_channels</code></strong>:<code>int</code>, <strong><code>time_channels</code></strong>:<code>int</code>, <strong><code>has_attn</code></strong>:<code>bool</code>) :: <code>Module</code></p>
</blockquote>
<h3 id="Down-block">Down block<a class="anchor-link" href="#Down-block"> </a></h3><p>This combines <a href="/cclddg/core.html#ResidualBlock"><code>ResidualBlock</code></a> and <a href="/cclddg/core.html#AttentionBlock"><code>AttentionBlock</code></a>. These are used in the first half of U-Net at each resolution.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="UpBlock" class="doc_header"><code>class</code> <code>UpBlock</code><a href="https://github.com/johnowhitaker/cclddg/tree/master/cclddg/core.py#L235" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>UpBlock</code>(<strong><code>in_channels</code></strong>:<code>int</code>, <strong><code>out_channels</code></strong>:<code>int</code>, <strong><code>time_channels</code></strong>:<code>int</code>, <strong><code>has_attn</code></strong>:<code>bool</code>) :: <code>Module</code></p>
</blockquote>
<h3 id="Up-block">Up block<a class="anchor-link" href="#Up-block"> </a></h3><p>This combines <a href="/cclddg/core.html#ResidualBlock"><code>ResidualBlock</code></a> and <a href="/cclddg/core.html#AttentionBlock"><code>AttentionBlock</code></a>. These are used in the second half of U-Net at each resolution.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="MiddleBlock" class="doc_header"><code>class</code> <code>MiddleBlock</code><a href="https://github.com/johnowhitaker/cclddg/tree/master/cclddg/core.py#L257" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>MiddleBlock</code>(<strong><code>n_channels</code></strong>:<code>int</code>, <strong><code>time_channels</code></strong>:<code>int</code>) :: <code>Module</code></p>
</blockquote>
<h3 id="Middle-block">Middle block<a class="anchor-link" href="#Middle-block"> </a></h3><p>It combines a <a href="/cclddg/core.html#ResidualBlock"><code>ResidualBlock</code></a>, <a href="/cclddg/core.html#AttentionBlock"><code>AttentionBlock</code></a>, followed by another <a href="/cclddg/core.html#ResidualBlock"><code>ResidualBlock</code></a>.
This block is applied at the lowest resolution of the U-Net.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Upsample" class="doc_header"><code>class</code> <code>Upsample</code><a href="https://github.com/johnowhitaker/cclddg/tree/master/cclddg/core.py#L277" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Upsample</code>(<strong><code>n_channels</code></strong>) :: <code>Module</code></p>
</blockquote>
<h3 id="Scale-up-the-feature-map-by-$2--imes$">Scale up the feature map by $2  imes$<a class="anchor-link" href="#Scale-up-the-feature-map-by-$2--imes$"> </a></h3>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Downsample" class="doc_header"><code>class</code> <code>Downsample</code><a href="https://github.com/johnowhitaker/cclddg/tree/master/cclddg/core.py#L293" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Downsample</code>(<strong><code>n_channels</code></strong>) :: <code>Module</code></p>
</blockquote>
<h3>Scale down the feature map by $rac{1}{2}       imes$</h3>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-UNet-and-Discriminator">The UNet and Discriminator<a class="anchor-link" href="#The-UNet-and-Discriminator"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="UNet" class="doc_header"><code>class</code> <code>UNet</code><a href="https://github.com/johnowhitaker/cclddg/tree/master/cclddg/core.py#L313" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>UNet</code>(<strong><code>image_channels</code></strong>:<code>int</code>=<em><code>3</code></em>, <strong><code>n_channels</code></strong>:<code>int</code>=<em><code>64</code></em>, <strong><code>ch_mults</code></strong>:<code>Union</code>[<code>typing.Tuple[int, ...]</code>, <code>typing.List[int]</code>]=<em><code>(1, 2, 2, 4)</code></em>, <strong><code>is_attn</code></strong>:<code>Union</code>[<code>typing.Tuple[bool, ...]</code>, <code>typing.List[int]</code>]=<em><code>(False, False, True, True)</code></em>, <strong><code>n_blocks</code></strong>:<code>int</code>=<em><code>2</code></em>) :: <code>Module</code></p>
</blockquote>
<h2 id="U-Net">U-Net<a class="anchor-link" href="#U-Net"> </a></h2>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="DISC" class="doc_header"><code>class</code> <code>DISC</code><a href="https://github.com/johnowhitaker/cclddg/tree/master/cclddg/core.py#L449" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>DISC</code>(<strong><code>image_channels</code></strong>:<code>int</code>=<em><code>3</code></em>, <strong><code>n_channels</code></strong>:<code>int</code>=<em><code>64</code></em>, <strong><code>ch_mults</code></strong>:<code>Union</code>[<code>typing.Tuple[int, ...]</code>, <code>typing.List[int]</code>]=<em><code>(1, 2, 2, 4)</code></em>, <strong><code>is_attn</code></strong>:<code>Union</code>[<code>typing.Tuple[bool, ...]</code>, <code>typing.List[int]</code>]=<em><code>(False, False, True, True)</code></em>, <strong><code>n_blocks</code></strong>:<code>int</code>=<em><code>2</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>.. note::
    As per the example above, an <code>__init__()</code> call to the parent class
    must be made before assignment on the child.</p>
<p>:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">device</span> <span class="o">=</span> <span class="s1">&#39;cpu&#39;</span>
<span class="n">unet</span> <span class="o">=</span> <span class="n">UNet</span><span class="p">(</span><span class="n">image_channels</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">8</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">512</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">pred_im</span> <span class="o">=</span> <span class="n">unet</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">t</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
<span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">pred_im</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([1, 4, 16, 16]), torch.Size([1, 4, 16, 16]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

