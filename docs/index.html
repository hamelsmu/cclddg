---

title: CLOOB Conditioned Latent Denoising Diffusion GAN


keywords: fastai
sidebar: home_sidebar

summary: "My code and utilities for training CCLDDGs."
description: "My code and utilities for training CCLDDGs."
nb_path: "index.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: index.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This is very much a work in progress. Stay tuned for better info soon :)</p>
<p>In the meantime, the core ideas:</p>
<ul>
<li>Diffusion models gradually noise an image and learn the reverse process. Great but lots of steps means slow inference</li>
<li>DDGs tweak the training to include a discriminator, and promise good results in very few steps at inference</li>
<li>Latent diffusion means doing it in the latent space of some autoencoder, which is more efficient.</li>
<li>We can condition these models with various extra info. I chose to use CLOOB embeddings as conditioning info. That way we can get these from images or text or both during training, and at inference feed in either text or images and use them as conditioning for the generation process. In theory this gives a nice way to do text-to-image!</li>
</ul>
<p>I tried training for a few hours on CC12M and while the results aren't photorealistic you can definitely see <em>something</em> of a prompt in the outputs - 'blue ocean waves' is mostly blue, for eg. Results from a longer trainging run soon.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Install">Install<a class="anchor-link" href="#Install"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>I might turn this into a package later, for now your best bet is to check out the colab(s) below or follow the instructions in 'Running the training script'.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="What-is-all-this">What is all this<a class="anchor-link" href="#What-is-all-this"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The main thing this code does is define a UNet architecture and an accompanying Discriminator architecture that can take in an image (or a latent representation of one) along with conditioning information (what timestep we're looking at, a CLOOB embedding of an image or caption) and a latent variable <code>z</code> used to turn the unet into a more GAN-like multimodal generator thingee.</p>
<p>Demos</p>
<ul>
<li>A standard diffusion model TODO</li>
<li>A standard latent diffusion model TODO</li>
<li>A standard Defusion Denoising GAN TODO</li>
<li>CLOOB-Conditioned Latent Defusion Denoising GAN: <a href="https://colab.research.google.com/drive/1T5LommNOw4cVr8bX6AO5QXJ7D1LyXz2m?usp=sharing">https://colab.research.google.com/drive/1T5LommNOw4cVr8bX6AO5QXJ7D1LyXz2m?usp=sharing</a> (faces)</li>
</ul>
<p>W&amp;B runs TODO</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Running-the-training-script">Running the training script<a class="anchor-link" href="#Running-the-training-script"> </a></h1><p>The train script is written to run OUTSIDE this directory (aka NOT in cclddg). It also assumes the locations of various dependancies and model files. To set it up, in a notebook run:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># !git clone https://github.com/CompVis/latent-diffusion                           &amp;&gt;&gt; install.log</span>
<span class="c1"># !git clone https://github.com/CompVis/taming-transformers                        &amp;&gt;&gt; install.log</span>
<span class="c1"># !pip install -e ./taming-transformers                                            &amp;&gt;&gt; install.log</span>
<span class="c1"># !git clone --recursive https://github.com/crowsonkb/cloob-training               &amp;&gt;&gt; install.log</span>
<span class="c1"># !git clone https://github.com/openai/CLIP/                                       &amp;&gt;&gt; install.log</span>
<span class="c1"># !pip install CLIP/.                                                              &amp;&gt;&gt; install.log</span>
<span class="c1"># !pip install --upgrade webdataset ipywidgets lpips                               &amp;&gt;&gt; install.log</span>
<span class="c1"># !pip install datasets omegaconf einops wandb pytorch_lightning                   &amp;&gt;&gt; install.log</span>
<span class="c1"># !wget https://ommer-lab.com/files/latent-diffusion/kl-f8.zip                     &amp;&gt;&gt; install.log</span>
<span class="c1"># !unzip -q kl-f8.zip </span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Then if you wish to use W&amp;B for logging, run <code>wandb login</code> in a terminal.</p>
<p>Then copy the script from the cclddg folder downloaded as part of the command above to your local dir:</p>
<p><code>cp cclddg/train_cclddg.py train.py</code></p>
<p>And run your training like so:</p>
<p><code>python train.py --z_dim 16 --n_channels_unet 64 --batch_size 64 --n_batches 200 --lr_gen 0.0001 --lr_disc 0.0001 --log_images_every 50 --save_models_every 500 --n_steps 8 --dataset celebA --wandb_project cclddg_faces</code></p>

</div>
</div>
</div>
</div>
 

