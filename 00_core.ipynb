{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCLDDG Core Class\n",
    "> Defining the key UNet and Discriminator architectures used.\n",
    "\n",
    "I've tried to make this as informative as possible :)\n",
    "\n",
    "I should mention that I started with code from [the labml annotated DDPM paper](https://github.com/labmlai/annotated_deep_learning_paper_implementations/tree/master/labml_nn/diffusion/ddpm), so credit there! It's a great resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import math\n",
    "from typing import Optional, Tuple, Union, List\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Blocks\n",
    "\n",
    "This section defines the different building blocks we'll use to build the core unet and discriminator architectures.\n",
    "\n",
    "### The activation function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default this all uses the `Swish` activation function: $x \\cdot \\sigma(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Swish(nn.Module):\n",
    "    \"\"\" *swish...* \"\"\"\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of this as 'fancy ReLU'... This is what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#collapse\n",
    "swish = Swish()\n",
    "x = torch.linspace(-8, 5, 100)\n",
    "y = swish(x)\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want a way to create embeddings from various conditioning information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# The time embedding \n",
    "class TimeEmbedding(nn.Module):\n",
    "    \"\"\" Embeddings for $t$ \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels: int, denom_factor=10000):\n",
    "        \"\"\"\n",
    "        * `n_channels` is the number of dimensions in the embedding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.denom_factor = denom_factor\n",
    "        # First linear layer\n",
    "        self.lin1 = nn.Linear(self.n_channels // 4, self.n_channels)\n",
    "        # Activation\n",
    "        self.act = Swish()\n",
    "        # Second linear layer\n",
    "        self.lin2 = nn.Linear(self.n_channels, self.n_channels)\n",
    "\n",
    "    def forward(self, t: torch.Tensor, return_sinusoidal_embs=False):\n",
    "        \"\"\" Create sinusoidal position embeddings as used in many transformers. \"\"\"\n",
    "        half_dim = self.n_channels // 8\n",
    "        emb = math.log(self.denom_factor) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=1)\n",
    "        \n",
    "        if return_sinusoidal_embs:\n",
    "            return emb\n",
    "        \n",
    "\n",
    "        # Transform with the MLP\n",
    "        emb = self.act(self.lin1(emb))\n",
    "        emb = self.lin2(emb)\n",
    "\n",
    "        #\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Positional Embedding used for `TimeEmbedding` is a sinusoidal embedding as used in many transformer implementations:\n",
    "$$\n",
    "\\begin{align}\n",
    "PE^{(1)}_{t,i} &= sin\\Bigg(\\frac{t}{10000^{\\frac{i}{d - 1}}}\\Bigg) \\\\\n",
    "PE^{(2)}_{t,i} &= cos\\Bigg(\\frac{t}{10000^{\\frac{i}{d - 1}}}\\Bigg)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $d$ is `half_dim` = `n_channels//8`\n",
    "\n",
    "Since we expect to encode only a small number of steps we can specify a smaller multiplier than 10000 using the `denom_factor` argument.\n",
    "\n",
    "These sinusoidal embeddings are usually then passed through an MLP to transform them into n_channels outputs, but we can pass in `return_sinusoidal_embs=True` to get the raw sinusoidal embeddings for visualization purposes. Here's an example visualizing this for t in range(0, 10):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#collapse\n",
    "te = TimeEmbedding(n_channels=64, denom_factor=16)\n",
    "t = torch.arange(0, 10)\n",
    "embs = te(t, return_sinusoidal_embs=True)\n",
    "embs.shape\n",
    "plt.imshow(embs.detach(), )\n",
    "plt.xlabel('Sinusoidal Encodings (sin component then cos)')\n",
    "plt.ylabel('Input (t)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also create embeddings to map a latent variable `z` and our CLOOB embedding to set numbers of channels. Both simply run the input through a small MLP to map them to `n_channels` outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# CLOOB embedding\n",
    "class CLOOBEmbedding(nn.Module):\n",
    "    \"\"\" Embedding to map a CLOOB embedding (512 dimensions) to n_channels via an MLP \"\"\"\n",
    "    def __init__(self, n_channels: int):\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.lin1 = nn.Linear(512, self.n_channels)\n",
    "        self.act = Swish()\n",
    "        self.lin2 = nn.Linear(self.n_channels, self.n_channels)\n",
    "\n",
    "    def forward(self, emb):\n",
    "        # Transform with the MLP\n",
    "        emb = self.act(self.lin1(emb))\n",
    "        emb = self.lin2(emb)\n",
    "        return emb\n",
    "\n",
    "# One for the gan latent, z\n",
    "class ZEmbedding(nn.Module):\n",
    "    \"\"\" Embedding to map a latent z (`z_dim` dimensions) to n_channels via an MLP \"\"\"\n",
    "    def __init__(self, z_dim:int, n_channels: int):\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.z_dim = z_dim\n",
    "        self.lin1 = nn.Linear(z_dim, self.n_channels)\n",
    "        self.act = Swish()\n",
    "        self.lin2 = nn.Linear(self.n_channels, self.n_channels)\n",
    "\n",
    "    def forward(self, emb):\n",
    "        # Transform with the MLP\n",
    "        emb = self.act(self.lin1(emb))\n",
    "        emb = self.lin2(emb)\n",
    "        return emb\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the building blocks are fairly standard, but all here take both an input (x) and some conditioning (cond). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Residual blocks include 'skip' connections\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Residual block\n",
    "    A residual block has two convolution layers with group normalization.\n",
    "    Each resolution is processed with two residual blocks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, n_cond_channels: int, n_groups: int = 32):\n",
    "        \"\"\"\n",
    "        * `in_channels` is the number of input channels\n",
    "        * `out_channels` is the number of input channels\n",
    "        * `time_channels` is the number channels in the time step ($t$) embeddings\n",
    "        * `n_groups` is the number of groups for [group normalization](../../normalization/group_norm/index.html)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Group normalization and the first convolution layer\n",
    "        self.norm1 = nn.GroupNorm(n_groups, in_channels)\n",
    "        self.act1 = Swish()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), padding=(1, 1))\n",
    "\n",
    "        # Group normalization and the second convolution layer\n",
    "        self.norm2 = nn.GroupNorm(n_groups, out_channels)\n",
    "        self.act2 = Swish()\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=(3, 3), padding=(1, 1))\n",
    "\n",
    "        # If the number of input channels is not equal to the number of output channels we have to\n",
    "        # project the shortcut connection\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=(1, 1))\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "        # Linear layer for conditional embeddings\n",
    "        self.cond_emb = nn.Linear(n_cond_channels, out_channels)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, cond: torch.Tensor):\n",
    "        \"\"\"\n",
    "        * `x` has shape `[batch_size, in_channels, height, width]`\n",
    "        * `t` has shape `[batch_size, time_channels]`\n",
    "        \"\"\"\n",
    "        # First convolution layer\n",
    "        h = self.conv1(self.act1(self.norm1(x)))\n",
    "        # Add time embeddings\n",
    "        h += self.cond_emb(cond)[:, :, None, None]\n",
    "        # Second convolution layer\n",
    "        h = self.conv2(self.act2(self.norm2(h)))\n",
    "\n",
    "        # Add the shortcut connection and return\n",
    "        return h + self.shortcut(x)\n",
    "\n",
    "# Ahh yes, magical attention...\n",
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Attention block\n",
    "    This is similar to [transformer multi-head attention](../../transformers/mha.html).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels: int, n_heads: int = 1, d_k: int = None, n_groups: int = 32):\n",
    "        \"\"\"\n",
    "        * `n_channels` is the number of channels in the input\n",
    "        * `n_heads` is the number of heads in multi-head attention\n",
    "        * `d_k` is the number of dimensions in each head\n",
    "        * `n_groups` is the number of groups for [group normalization](../../normalization/group_norm/index.html)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Default `d_k`\n",
    "        if d_k is None:\n",
    "            d_k = n_channels\n",
    "        # Normalization layer\n",
    "        self.norm = nn.GroupNorm(n_groups, n_channels)\n",
    "        # Projections for query, key and values\n",
    "        self.projection = nn.Linear(n_channels, n_heads * d_k * 3)\n",
    "        # Linear layer for final transformation\n",
    "        self.output = nn.Linear(n_heads * d_k, n_channels)\n",
    "        # Scale for dot-product attention\n",
    "        self.scale = d_k ** -0.5\n",
    "        #\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_k\n",
    "\n",
    "    def forward(self, x: torch.Tensor, cond: Optional[torch.Tensor] = None):\n",
    "        \"\"\"\n",
    "        * `x` has shape `[batch_size, in_channels, height, width]`\n",
    "        * `t` has shape `[batch_size, time_channels]`\n",
    "        \"\"\"\n",
    "        # `cond` is not used, but it's kept in the arguments because for the attention layer function signature\n",
    "        # to match with `ResidualBlock`.\n",
    "        _ = cond\n",
    "        # Get shape\n",
    "        batch_size, n_channels, height, width = x.shape\n",
    "        # Change `x` to shape `[batch_size, seq, n_channels]`\n",
    "        x = x.view(batch_size, n_channels, -1).permute(0, 2, 1)\n",
    "        # Get query, key, and values (concatenated) and shape it to `[batch_size, seq, n_heads, 3 * d_k]`\n",
    "        qkv = self.projection(x).view(batch_size, -1, self.n_heads, 3 * self.d_k)\n",
    "        # Split query, key, and values. Each of them will have shape `[batch_size, seq, n_heads, d_k]`\n",
    "        q, k, v = torch.chunk(qkv, 3, dim=-1)\n",
    "        # Calculate scaled dot-product $\\frac{Q K^\\top}{\\sqrt{d_k}}$\n",
    "        attn = torch.einsum('bihd,bjhd->bijh', q, k) * self.scale\n",
    "        # Softmax along the sequence dimension $\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigg)$\n",
    "        attn = attn.softmax(dim=1)\n",
    "        # Multiply by values\n",
    "        res = torch.einsum('bijh,bjhd->bihd', attn, v)\n",
    "        # Reshape to `[batch_size, seq, n_heads * d_k]`\n",
    "        res = res.view(batch_size, -1, self.n_heads * self.d_k)\n",
    "        # Transform to `[batch_size, seq, n_channels]`\n",
    "        res = self.output(res)\n",
    "\n",
    "        # Add skip connection\n",
    "        res += x\n",
    "\n",
    "        # Change to shape `[batch_size, in_channels, height, width]`\n",
    "        res = res.permute(0, 2, 1).view(batch_size, n_channels, height, width)\n",
    "\n",
    "        #\n",
    "        return res\n",
    "\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Down block\n",
    "    This combines `ResidualBlock` and `AttentionBlock`. These are used in the first half of U-Net at each resolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, time_channels: int, has_attn: bool):\n",
    "        super().__init__()\n",
    "        self.res = ResidualBlock(in_channels, out_channels, time_channels)\n",
    "        if has_attn:\n",
    "            self.attn = AttentionBlock(out_channels)\n",
    "        else:\n",
    "            self.attn = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, cond: torch.Tensor):\n",
    "        x = self.res(x, cond)\n",
    "        x = self.attn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Up block\n",
    "    This combines `ResidualBlock` and `AttentionBlock`. These are used in the second half of U-Net at each resolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, n_cond_channels: int, has_attn: bool):\n",
    "        super().__init__()\n",
    "        # The input has `in_channels + out_channels` because we concatenate the output of the same resolution\n",
    "        # from the first half of the U-Net\n",
    "        self.res = ResidualBlock(in_channels + out_channels, out_channels, n_cond_channels)\n",
    "        if has_attn:\n",
    "            self.attn = AttentionBlock(out_channels)\n",
    "        else:\n",
    "            self.attn = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, cond: torch.Tensor):\n",
    "        x = self.res(x, cond)\n",
    "        x = self.attn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MiddleBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Middle block\n",
    "    It combines a `ResidualBlock`, `AttentionBlock`, followed by another `ResidualBlock`.\n",
    "    This block is applied at the lowest resolution of the U-Net.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels: int, n_cond_channels: int):\n",
    "        super().__init__()\n",
    "        self.res1 = ResidualBlock(n_channels, n_channels, n_cond_channels)\n",
    "        self.attn = AttentionBlock(n_channels)\n",
    "        self.res2 = ResidualBlock(n_channels, n_channels, n_cond_channels)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, cond: torch.Tensor):\n",
    "        x = self.res1(x, cond)\n",
    "        x = self.attn(x)\n",
    "        x = self.res2(x, cond)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Scale up the feature map by 2.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.ConvTranspose2d(n_channels, n_channels, (4, 4), (2, 2), (1, 1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, cond: torch.Tensor):\n",
    "        # `cond` is not used, but it's kept in the arguments because for the attention layer function signature\n",
    "        # to match with `ResidualBlock`.\n",
    "        _ = cond\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Scale down the feature map by 0.5\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(n_channels, n_channels, (3, 3), (2, 2), (1, 1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, cond: torch.Tensor):\n",
    "        # `cond` is not used, but it's kept in the arguments because for the attention layer function signature\n",
    "        # to match with `ResidualBlock`.\n",
    "        _ = cond\n",
    "        return self.conv(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The UNet and Discriminator\n",
    "\n",
    "This is what we've been building up to. We want a UNet mode that can take in a (noisy) image or image-like tensor, along with some conditioning information (timestep, CLOOB embedding) and optionally a latent `z`, and produce an output of the same shape as the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# The core class definition (aka the important bit)\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    #### U-Net\n",
    "    \n",
    "    Hopefully flexible enough :) Arguments:\n",
    "    \n",
    "        * `image_channels` is the number of channels in the image. $3$ for RGB.\n",
    "        * `n_channels` is number of channels in the initial feature map that we transform the image into\n",
    "        * `ch_mults` is the list of channel numbers at each resolution. The number of channels is `ch_mults[i] * n_channels`\n",
    "        * `is_attn` is a list of booleans that indicate whether to use attention at each resolution\n",
    "        * `n_blocks` is the number of `UpDownBlocks` at each resolution\n",
    "        * `use_z`=True. Set to false if you don't want to include the latent z input\n",
    "        * `z_dim` is the dimension of the latent `z`, and `n_z_channels` is the size of the embedding used for it.\n",
    "        * `use_cloob` = True. Set to false if you don't want to use CLOOB conditioning.\n",
    "        * `n_cloob_channels` - the size of the embedding used for the CLOOB conditioning input.\n",
    "        * `n_time_channels` - the size of the time embedding. If -1, this is set to n_channels*4\n",
    "        * `denom_factor` for the TimeEmbedding. 100 by default, set to 10,000 if wanting to do more traditional diffusion stuff where n_steps is high.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_channels: int = 3, n_channels: int = 64,\n",
    "                 ch_mults: Union[Tuple[int, ...], List[int]] = (1, 2, 2, 4),\n",
    "                 is_attn: Union[Tuple[bool, ...], List[int]] = (False, False, True, True),\n",
    "                 n_blocks: int = 2,\n",
    "                 use_z=True,z_dim: int = 8, n_z_channels: int=16,\n",
    "                 use_cloob=True, n_cloob_channels: int = 256,\n",
    "                 n_time_channels: int=-1,\n",
    "                 denom_factor: int=100):\n",
    "        super().__init__()\n",
    "\n",
    "        # Number of resolutions\n",
    "        n_resolutions = len(ch_mults)\n",
    "\n",
    "        # Project image into feature map\n",
    "        self.image_proj = nn.Conv2d(image_channels, n_channels, kernel_size=(3, 3), padding=(1, 1))\n",
    "\n",
    "\n",
    "        # Time embedding layer. Time embedding has `n_channels * 4` channels in the original paper, with demon_factor=10,000\n",
    "        n_time_channels = n_channels * 4\n",
    "        self.time_emb = TimeEmbedding(n_time_channels, denom_factor=denom_factor)\n",
    "        \n",
    "        # CLOOB embeddings\n",
    "        self.use_cloob = use_cloob\n",
    "        self.cloob_emb = CLOOBEmbedding(n_cloob_channels)\n",
    "\n",
    "        # Z embeddings\n",
    "        self.use_z = use_z\n",
    "        self.z_emb = ZEmbedding(z_dim=z_dim, n_channels=n_z_channels)\n",
    "        \n",
    "        n_cond_channels = n_time_channels\n",
    "        if use_cloob:\n",
    "            n_cond_channels += n_cloob_channels\n",
    "        if use_z:\n",
    "            n_cond_channels += n_z_channels\n",
    "        \n",
    "        # #### First half of U-Net - decreasing resolution\n",
    "        down = []\n",
    "        # Number of channels\n",
    "        out_channels = in_channels = n_channels\n",
    "        # For each resolution\n",
    "        for i in range(n_resolutions):\n",
    "            # Number of output channels at this resolution\n",
    "            out_channels = in_channels * ch_mults[i]\n",
    "            # Add `n_blocks`\n",
    "            for _ in range(n_blocks):\n",
    "                down.append(DownBlock(in_channels, out_channels, n_cond_channels, is_attn[i]))\n",
    "                in_channels = out_channels\n",
    "            # Down sample at all resolutions except the last\n",
    "            if i < n_resolutions - 1:\n",
    "                down.append(Downsample(in_channels))\n",
    "\n",
    "        # Combine the set of modules\n",
    "        self.down = nn.ModuleList(down)\n",
    "\n",
    "        # Middle block\n",
    "        self.middle = MiddleBlock(out_channels, n_cond_channels, )\n",
    "\n",
    "        # #### Second half of U-Net - increasing resolution\n",
    "        up = []\n",
    "        # Number of channels\n",
    "        in_channels = out_channels\n",
    "        # For each resolution\n",
    "        for i in reversed(range(n_resolutions)):\n",
    "            # `n_blocks` at the same resolution\n",
    "            out_channels = in_channels\n",
    "            for _ in range(n_blocks):\n",
    "                up.append(UpBlock(in_channels, out_channels, n_cond_channels, is_attn[i]))\n",
    "            # Final block to reduce the number of channels\n",
    "            out_channels = in_channels // ch_mults[i]\n",
    "            up.append(UpBlock(in_channels, out_channels, n_cond_channels, is_attn[i]))\n",
    "            in_channels = out_channels\n",
    "            # Up sample at all resolutions except last\n",
    "            if i > 0:\n",
    "                up.append(Upsample(in_channels))\n",
    "\n",
    "        # Combine the set of modules\n",
    "        self.up = nn.ModuleList(up)\n",
    "\n",
    "        # Final normalization and convolution layer\n",
    "        self.norm = nn.GroupNorm(8, n_channels)\n",
    "        self.act = Swish()\n",
    "        self.final = nn.Conv2d(in_channels, image_channels, kernel_size=(3, 3), padding=(1, 1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor, c=None, z=None):\n",
    "        \"\"\"\n",
    "        * `x` has shape `[batch_size, in_channels, height, width]`\n",
    "        * `t` has shape `[batch_size]`\n",
    "        * `c` is cloob embeddings shape [batch_size, 512]\n",
    "        * `z` is latent input shape [batch_size, z_dim]\n",
    "        \"\"\"\n",
    "        \n",
    "        # The combined embeddings become our conditioning info (n_cond_channels total) fed in at various stages.\n",
    "        # Starting with the time-step embeddings\n",
    "        t = self.time_emb(t)\n",
    "        cond = t\n",
    "        \n",
    "        # Combine with the cloob & z embeddings (if applicable)\n",
    "        if self.use_cloob:\n",
    "            if c==None:\n",
    "                c = torch.zeros((x.shape[0], 512)).to(t.device)\n",
    "            c = self.cloob_emb(c)\n",
    "            cond = torch.cat((cond, c), dim=1)\n",
    "        if self.use_z:\n",
    "            if z==None:\n",
    "                z = torch.zeros((x.shape[0], self.z_emb.z_dim)).to(t.device)\n",
    "            z = self.z_emb(z)\n",
    "            cond = torch.cat((cond, z), dim=1)\n",
    "\n",
    "        # Get image projection\n",
    "        x = self.image_proj(x)\n",
    "\n",
    "        # `h` will store outputs at each resolution for skip connection\n",
    "        h = [x]\n",
    "        # First half of U-Net\n",
    "        for m in self.down:\n",
    "            x = m(x, cond)\n",
    "            h.append(x)\n",
    "\n",
    "        # Middle (bottom)\n",
    "        x = self.middle(x, cond)\n",
    "\n",
    "        # Second half of U-Net\n",
    "        for m in self.up:\n",
    "            if isinstance(m, Upsample):\n",
    "                x = m(x, cond)\n",
    "            else:\n",
    "                # Get the skip connection from first half of U-Net and concatenate\n",
    "                s = h.pop()\n",
    "                x = torch.cat((x, s), dim=1)\n",
    "                # Add in conditioning\n",
    "                x = m(x, cond)\n",
    "\n",
    "        # Final normalization and convolution\n",
    "        return self.final(self.act(self.norm(x)))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd also like a Discriminator that can take in an image, with the same optional conditioning information, and spit out a classification (real or fake). If you want to condition the discriminator on another image (e.g. in DDG the discriminator takes in $x_{t-1}$ and is conditioned on $x_t$) then simply concatenate them together and use `image_channels = 2*[the number of channels in a single image]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "    #### Discriminator\n",
    "    \n",
    "    Based on the same architecture as the UNet, but without the upwards half. Arguments:\n",
    "    \n",
    "        * `image_channels` is the number of channels in the image. $3$ for RGB.\n",
    "        * `n_channels` is number of channels in the initial feature map that we transform the image into\n",
    "        * `ch_mults` is the list of channel numbers at each resolution. The number of channels is `ch_mults[i] * n_channels`\n",
    "        * `is_attn` is a list of booleans that indicate whether to use attention at each resolution\n",
    "        * `n_blocks` is the number of `UpDownBlocks` at each resolution\n",
    "        * `use_cloob` = True. Set to false if you don't want to use CLOOB conditioning.\n",
    "        * `n_cloob_channels` - the size of the embedding used for the CLOOB conditioning input.\n",
    "        * `n_time_channels` - the size of the time embedding. If -1, this is set to n_channels*4\n",
    "        * `denom_factor` for the TimeEmbedding. 100 by default, set to 10,000 if wanting to do more traditional diffusion stuff where n_steps is high.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, image_channels: int = 3, n_channels: int = 64,\n",
    "                 ch_mults: Union[Tuple[int, ...], List[int]] = (1, 2, 2, 4),\n",
    "                 is_attn: Union[Tuple[bool, ...], List[int]] = (False, False, True, True),\n",
    "                 n_blocks: int = 2,\n",
    "                 use_cloob=True, n_cloob_channels: int = 256,\n",
    "                 n_time_channels: int=-1,\n",
    "                 denom_factor: int=100):\n",
    "        super().__init__()\n",
    "\n",
    "        # Number of resolutions\n",
    "        n_resolutions = len(ch_mults)\n",
    "\n",
    "        # Project image into feature map\n",
    "        self.image_proj = nn.Conv2d(image_channels, n_channels, kernel_size=(3, 3), padding=(1, 1))\n",
    "\n",
    "       # Time embedding layer. Time embedding has `n_channels * 4` channels in the original paper, with demon_factor=10,000\n",
    "        n_time_channels = n_channels * 4\n",
    "        self.time_emb = TimeEmbedding(n_time_channels, denom_factor=denom_factor)\n",
    "        \n",
    "        # CLOOB embeddings\n",
    "        self.use_cloob = use_cloob\n",
    "        self.cloob_emb = CLOOBEmbedding(n_cloob_channels)\n",
    "        \n",
    "        n_cond_channels = n_time_channels\n",
    "        if use_cloob:\n",
    "            n_cond_channels += n_cloob_channels\n",
    "        \n",
    "        # #### First half (same as Unet)- decreasing resolution\n",
    "        down = []\n",
    "        # Number of channels\n",
    "        out_channels = in_channels = n_channels\n",
    "        # For each resolution\n",
    "        for i in range(n_resolutions):\n",
    "            # Number of output channels at this resolution\n",
    "            out_channels = in_channels * ch_mults[i]\n",
    "            # Add `n_blocks`\n",
    "            for _ in range(n_blocks):\n",
    "                down.append(DownBlock(in_channels, out_channels, n_cond_channels, is_attn[i]))\n",
    "                in_channels = out_channels\n",
    "            # Down sample at all resolutions except the last\n",
    "            if i < n_resolutions - 1:\n",
    "                down.append(Downsample(in_channels))\n",
    "\n",
    "        # Combine the set of modules\n",
    "        self.down = nn.ModuleList(down)\n",
    "\n",
    "        # Middle block\n",
    "        self.middle = MiddleBlock(out_channels, n_cond_channels, )\n",
    "\n",
    "        # Final 'head'\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(out_channels, 1), # TODO add a second MLP layer here maybe?\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor, c=None):\n",
    "        \"\"\"\n",
    "        * `x` has shape `[batch_size, in_channels, height, width]`\n",
    "        * `t` has shape `[batch_size]`\n",
    "        * `c` is cloob embeddings shape [batch_size, 512]\n",
    "        \"\"\"\n",
    "\n",
    "       # The combined embeddings become our conditioning info (n_cond_channels total) fed in at various stages.\n",
    "        # Starting with the time-step embeddings\n",
    "        t = self.time_emb(t)\n",
    "        cond = t\n",
    "        \n",
    "        # Combine with the cloob if applicable\n",
    "        if self.use_cloob:\n",
    "            if c==None:\n",
    "                c = torch.zeros((x.shape[0], 512))\n",
    "            c = self.cloob_emb(c)\n",
    "            cond = torch.cat((cond, c), dim=1)\n",
    "\n",
    "        # Get image projection\n",
    "        x = self.image_proj(x)\n",
    "\n",
    "        # `h` will store outputs at each resolution for skip connection\n",
    "        h = [x]\n",
    "        # First half of U-Net\n",
    "        for m in self.down:\n",
    "            x = m(x, cond)\n",
    "            h.append(x)\n",
    "\n",
    "        # Middle (bottom)\n",
    "        x = self.middle(x, cond)\n",
    "\n",
    "        return self.head(x) # Final output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see both in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 16, 16]), torch.Size([1, 4, 16, 16]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "unet = UNet(image_channels=4).to(device)\n",
    "z = torch.randn((1,8), device=device)\n",
    "c = torch.zeros((1,512), device=device)\n",
    "x = torch.randn(1, 4, 16, 16).to(device)\n",
    "t = torch.tensor(3, dtype=torch.long).unsqueeze(0).to(device)\n",
    "pred_im = unet(x.float(), t, c, z)\n",
    "x.shape, pred_im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disc = Discriminator(image_channels=4, use_cloob=False)\n",
    "disc(x, t).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6967, grad_fn=<BinaryCrossEntropyBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = torch.tensor([1]).float()\n",
    "criterion = nn.BCELoss()\n",
    "criterion(disc(x, t).view(-1), labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
