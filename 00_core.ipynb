{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Title\n",
    "> API details.\n",
    "\n",
    "Messing about with the idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import math\n",
    "from typing import Optional, Tuple, Union, List\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Blocks\n",
    "\n",
    "This section defines the different building blocks we'll use to build the core unet and discriminator architectures.\n",
    "\n",
    "First up: the activation function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default this all uses the `Swish` activation function: $x \\cdot \\sigma(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "swish = Swish()\n",
    "x = torch.linspace(-8, 5, 100)\n",
    "y = swish(x)\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want a way to create embeddings from various conditioning information.\n",
    "\n",
    "Explain time embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# The time embedding \n",
    "class TimeEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Embeddings for $t$\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels: int):\n",
    "        \"\"\"\n",
    "        * `n_channels` is the number of dimensions in the embedding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        # First linear layer\n",
    "        self.lin1 = nn.Linear(self.n_channels // 4, self.n_channels)\n",
    "        # Activation\n",
    "        self.act = Swish()\n",
    "        # Second linear layer\n",
    "        self.lin2 = nn.Linear(self.n_channels, self.n_channels)\n",
    "\n",
    "    def forward(self, t: torch.Tensor):\n",
    "        # Create sinusoidal position embeddings\n",
    "        # [same as those from the transformer](../../transformers/positional_encoding.html)\n",
    "        #\n",
    "        # \\begin{align}\n",
    "        # PE^{(1)}_{t,i} &= sin\\Bigg(\\frac{t}{10000^{\\frac{i}{d - 1}}}\\Bigg) \\\\\n",
    "        # PE^{(2)}_{t,i} &= cos\\Bigg(\\frac{t}{10000^{\\frac{i}{d - 1}}}\\Bigg)\n",
    "        # \\end{align}\n",
    "        #\n",
    "        # where $d$ is `half_dim`\n",
    "        half_dim = self.n_channels // 8\n",
    "        emb = math.log(10_000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=1)\n",
    "\n",
    "        # Transform with the MLP\n",
    "        emb = self.act(self.lin1(emb))\n",
    "        emb = self.lin2(emb)\n",
    "\n",
    "        #\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"TimeEmbedding\" class=\"doc_header\"><code>class</code> <code>TimeEmbedding</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>TimeEmbedding</code>(**`n_channels`**:`int`) :: `Module`\n",
       "\n",
       "### Embeddings for $t$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TimeEmbedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "more text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# CLOOB embedding\n",
    "class CLOOBEmbedding(nn.Module):\n",
    "    def __init__(self, n_channels: int):\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.lin1 = nn.Linear(512, self.n_channels)\n",
    "        self.act = Swish()\n",
    "        self.lin2 = nn.Linear(self.n_channels, self.n_channels)\n",
    "\n",
    "    def forward(self, emb):\n",
    "        # Transform with the MLP\n",
    "        emb = self.act(self.lin1(emb))\n",
    "        emb = self.lin2(emb)\n",
    "        return emb\n",
    "\n",
    "# One for the gan latent, z\n",
    "class ZEmbedding(nn.Module):\n",
    "    def __init__(self, z_dim:int, n_channels: int):\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.lin1 = nn.Linear(z_dim, self.n_channels)\n",
    "        self.act = Swish()\n",
    "        self.lin2 = nn.Linear(self.n_channels, self.n_channels)\n",
    "\n",
    "    def forward(self, emb):\n",
    "        # Transform with the MLP\n",
    "        emb = self.act(self.lin1(emb))\n",
    "        emb = self.lin2(emb)\n",
    "        return emb\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the rest of the building blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Residual blocks include 'skip' connections\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Residual block\n",
    "    A residual block has two convolution layers with group normalization.\n",
    "    Each resolution is processed with two residual blocks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, time_channels: int, n_groups: int = 32):\n",
    "        \"\"\"\n",
    "        * `in_channels` is the number of input channels\n",
    "        * `out_channels` is the number of input channels\n",
    "        * `time_channels` is the number channels in the time step ($t$) embeddings\n",
    "        * `n_groups` is the number of groups for [group normalization](../../normalization/group_norm/index.html)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Group normalization and the first convolution layer\n",
    "        self.norm1 = nn.GroupNorm(n_groups, in_channels)\n",
    "        self.act1 = Swish()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), padding=(1, 1))\n",
    "\n",
    "        # Group normalization and the second convolution layer\n",
    "        self.norm2 = nn.GroupNorm(n_groups, out_channels)\n",
    "        self.act2 = Swish()\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=(3, 3), padding=(1, 1))\n",
    "\n",
    "        # If the number of input channels is not equal to the number of output channels we have to\n",
    "        # project the shortcut connection\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=(1, 1))\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "        # Linear layer for time embeddings\n",
    "        self.time_emb = nn.Linear(time_channels, out_channels)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        \"\"\"\n",
    "        * `x` has shape `[batch_size, in_channels, height, width]`\n",
    "        * `t` has shape `[batch_size, time_channels]`\n",
    "        \"\"\"\n",
    "        # First convolution layer\n",
    "        h = self.conv1(self.act1(self.norm1(x)))\n",
    "        # Add time embeddings\n",
    "        h += self.time_emb(t)[:, :, None, None]\n",
    "        # Second convolution layer\n",
    "        h = self.conv2(self.act2(self.norm2(h)))\n",
    "\n",
    "        # Add the shortcut connection and return\n",
    "        return h + self.shortcut(x)\n",
    "\n",
    "# Ahh yes, magical attention...\n",
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Attention block\n",
    "    This is similar to [transformer multi-head attention](../../transformers/mha.html).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels: int, n_heads: int = 1, d_k: int = None, n_groups: int = 32):\n",
    "        \"\"\"\n",
    "        * `n_channels` is the number of channels in the input\n",
    "        * `n_heads` is the number of heads in multi-head attention\n",
    "        * `d_k` is the number of dimensions in each head\n",
    "        * `n_groups` is the number of groups for [group normalization](../../normalization/group_norm/index.html)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Default `d_k`\n",
    "        if d_k is None:\n",
    "            d_k = n_channels\n",
    "        # Normalization layer\n",
    "        self.norm = nn.GroupNorm(n_groups, n_channels)\n",
    "        # Projections for query, key and values\n",
    "        self.projection = nn.Linear(n_channels, n_heads * d_k * 3)\n",
    "        # Linear layer for final transformation\n",
    "        self.output = nn.Linear(n_heads * d_k, n_channels)\n",
    "        # Scale for dot-product attention\n",
    "        self.scale = d_k ** -0.5\n",
    "        #\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_k\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: Optional[torch.Tensor] = None):\n",
    "        \"\"\"\n",
    "        * `x` has shape `[batch_size, in_channels, height, width]`\n",
    "        * `t` has shape `[batch_size, time_channels]`\n",
    "        \"\"\"\n",
    "        # `t` is not used, but it's kept in the arguments because for the attention layer function signature\n",
    "        # to match with `ResidualBlock`.\n",
    "        _ = t\n",
    "        # Get shape\n",
    "        batch_size, n_channels, height, width = x.shape\n",
    "        # Change `x` to shape `[batch_size, seq, n_channels]`\n",
    "        x = x.view(batch_size, n_channels, -1).permute(0, 2, 1)\n",
    "        # Get query, key, and values (concatenated) and shape it to `[batch_size, seq, n_heads, 3 * d_k]`\n",
    "        qkv = self.projection(x).view(batch_size, -1, self.n_heads, 3 * self.d_k)\n",
    "        # Split query, key, and values. Each of them will have shape `[batch_size, seq, n_heads, d_k]`\n",
    "        q, k, v = torch.chunk(qkv, 3, dim=-1)\n",
    "        # Calculate scaled dot-product $\\frac{Q K^\\top}{\\sqrt{d_k}}$\n",
    "        attn = torch.einsum('bihd,bjhd->bijh', q, k) * self.scale\n",
    "        # Softmax along the sequence dimension $\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigg)$\n",
    "        attn = attn.softmax(dim=1)\n",
    "        # Multiply by values\n",
    "        res = torch.einsum('bijh,bjhd->bihd', attn, v)\n",
    "        # Reshape to `[batch_size, seq, n_heads * d_k]`\n",
    "        res = res.view(batch_size, -1, self.n_heads * self.d_k)\n",
    "        # Transform to `[batch_size, seq, n_channels]`\n",
    "        res = self.output(res)\n",
    "\n",
    "        # Add skip connection\n",
    "        res += x\n",
    "\n",
    "        # Change to shape `[batch_size, in_channels, height, width]`\n",
    "        res = res.permute(0, 2, 1).view(batch_size, n_channels, height, width)\n",
    "\n",
    "        #\n",
    "        return res\n",
    "\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Down block\n",
    "    This combines `ResidualBlock` and `AttentionBlock`. These are used in the first half of U-Net at each resolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, time_channels: int, has_attn: bool):\n",
    "        super().__init__()\n",
    "        self.res = ResidualBlock(in_channels, out_channels, time_channels)\n",
    "        if has_attn:\n",
    "            self.attn = AttentionBlock(out_channels)\n",
    "        else:\n",
    "            self.attn = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        x = self.res(x, t)\n",
    "        x = self.attn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Up block\n",
    "    This combines `ResidualBlock` and `AttentionBlock`. These are used in the second half of U-Net at each resolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, time_channels: int, has_attn: bool):\n",
    "        super().__init__()\n",
    "        # The input has `in_channels + out_channels` because we concatenate the output of the same resolution\n",
    "        # from the first half of the U-Net\n",
    "        self.res = ResidualBlock(in_channels + out_channels, out_channels, time_channels)\n",
    "        if has_attn:\n",
    "            self.attn = AttentionBlock(out_channels)\n",
    "        else:\n",
    "            self.attn = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        x = self.res(x, t)\n",
    "        x = self.attn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MiddleBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Middle block\n",
    "    It combines a `ResidualBlock`, `AttentionBlock`, followed by another `ResidualBlock`.\n",
    "    This block is applied at the lowest resolution of the U-Net.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels: int, time_channels: int):\n",
    "        super().__init__()\n",
    "        self.res1 = ResidualBlock(n_channels, n_channels, time_channels)\n",
    "        self.attn = AttentionBlock(n_channels)\n",
    "        self.res2 = ResidualBlock(n_channels, n_channels, time_channels)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        x = self.res1(x, t)\n",
    "        x = self.attn(x)\n",
    "        x = self.res2(x, t)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Scale up the feature map by $2 \\times$\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.ConvTranspose2d(n_channels, n_channels, (4, 4), (2, 2), (1, 1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        # `t` is not used, but it's kept in the arguments because for the attention layer function signature\n",
    "        # to match with `ResidualBlock`.\n",
    "        _ = t\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Scale down the feature map by $\\frac{1}{2} \\times$\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(n_channels, n_channels, (3, 3), (2, 2), (1, 1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        # `t` is not used, but it's kept in the arguments because for the attention layer function signature\n",
    "        # to match with `ResidualBlock`.\n",
    "        _ = t\n",
    "        return self.conv(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The UNet and Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# The core class definition (aka the important bit)\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    ## U-Net\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_channels: int = 3, n_channels: int = 64,\n",
    "                 ch_mults: Union[Tuple[int, ...], List[int]] = (1, 2, 2, 4),\n",
    "                 is_attn: Union[Tuple[bool, ...], List[int]] = (False, False, True, True),\n",
    "                 n_blocks: int = 2):\n",
    "        \"\"\"\n",
    "        * `image_channels` is the number of channels in the image. $3$ for RGB.\n",
    "        * `n_channels` is number of channels in the initial feature map that we transform the image into\n",
    "        * `ch_mults` is the list of channel numbers at each resolution. The number of channels is `ch_mults[i] * n_channels`\n",
    "        * `is_attn` is a list of booleans that indicate whether to use attention at each resolution\n",
    "        * `n_blocks` is the number of `UpDownBlocks` at each resolution\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Number of resolutions\n",
    "        n_resolutions = len(ch_mults)\n",
    "\n",
    "        # Project image into feature map\n",
    "        self.image_proj = nn.Conv2d(image_channels, n_channels, kernel_size=(3, 3), padding=(1, 1))\n",
    "\n",
    "        # TODO n_time_channels, n_cloob_channels\n",
    "        # n_z_channels and z_dim should be args\n",
    "        # TODO both c and z should be optional\n",
    "\n",
    "        # Time embedding layer. Time embedding has `n_channels * 4` channels in the original, here 2 (to make room for cloob\n",
    "        n_time_channels = n_channels * 4\n",
    "        self.time_emb = TimeEmbedding(n_time_channels)\n",
    "        \n",
    "        # CLOOB embeddings\n",
    "        n_cloob_channels  = n_channels * 4\n",
    "        self.cloob_emb = CLOOBEmbedding(n_cloob_channels)\n",
    "\n",
    "        # Z embeddings\n",
    "        n_z_channels = 16 #128\n",
    "        self.z_emb = ZEmbedding(z_dim=8, n_channels=n_z_channels) # Change z_dim programatically (must also match disc?)\n",
    "        \n",
    "        n_cond_channels = n_time_channels+n_cloob_channels+n_z_channels\n",
    "        \n",
    "        # #### First half of U-Net - decreasing resolution\n",
    "        down = []\n",
    "        # Number of channels\n",
    "        out_channels = in_channels = n_channels\n",
    "        # For each resolution\n",
    "        for i in range(n_resolutions):\n",
    "            # Number of output channels at this resolution\n",
    "            out_channels = in_channels * ch_mults[i]\n",
    "            # Add `n_blocks`\n",
    "            for _ in range(n_blocks):\n",
    "                down.append(DownBlock(in_channels, out_channels, n_cond_channels, is_attn[i]))\n",
    "                in_channels = out_channels\n",
    "            # Down sample at all resolutions except the last\n",
    "            if i < n_resolutions - 1:\n",
    "                down.append(Downsample(in_channels))\n",
    "\n",
    "        # Combine the set of modules\n",
    "        self.down = nn.ModuleList(down)\n",
    "\n",
    "        # Middle block\n",
    "        self.middle = MiddleBlock(out_channels, n_cond_channels, )\n",
    "\n",
    "        # #### Second half of U-Net - increasing resolution\n",
    "        up = []\n",
    "        # Number of channels\n",
    "        in_channels = out_channels\n",
    "        # For each resolution\n",
    "        for i in reversed(range(n_resolutions)):\n",
    "            # `n_blocks` at the same resolution\n",
    "            out_channels = in_channels\n",
    "            for _ in range(n_blocks):\n",
    "                up.append(UpBlock(in_channels, out_channels, n_cond_channels, is_attn[i]))\n",
    "            # Final block to reduce the number of channels\n",
    "            out_channels = in_channels // ch_mults[i]\n",
    "            up.append(UpBlock(in_channels, out_channels, n_cond_channels, is_attn[i]))\n",
    "            in_channels = out_channels\n",
    "            # Up sample at all resolutions except last\n",
    "            if i > 0:\n",
    "                up.append(Upsample(in_channels))\n",
    "\n",
    "        # Combine the set of modules\n",
    "        self.up = nn.ModuleList(up)\n",
    "\n",
    "        # Final normalization and convolution layer\n",
    "        self.norm = nn.GroupNorm(8, n_channels)\n",
    "        self.act = Swish()\n",
    "        self.final = nn.Conv2d(in_channels, image_channels, kernel_size=(3, 3), padding=(1, 1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor, c:torch.Tensor, z:torch.Tensor):\n",
    "        \"\"\"\n",
    "        * `x` has shape `[batch_size, in_channels, height, width]`\n",
    "        * `t` has shape `[batch_size]`\n",
    "        \"\"\"\n",
    "\n",
    "        # Get time-step embeddings\n",
    "        t = self.time_emb(t)\n",
    "\n",
    "        # TODO handle when c and or z are None\n",
    "        # TODO both c and z should be optional\n",
    "        \n",
    "        # Combine with the cloob & z embeddings (hackity hack)\n",
    "        c = self.cloob_emb(c)\n",
    "        z = self.z_emb(z)\n",
    "        t = torch.cat((t, c, z), dim=1)\n",
    "\n",
    "        # Get image projection\n",
    "        x = self.image_proj(x)\n",
    "\n",
    "        # `h` will store outputs at each resolution for skip connection\n",
    "        h = [x]\n",
    "        # First half of U-Net\n",
    "        for m in self.down:\n",
    "            x = m(x, t)\n",
    "            h.append(x)\n",
    "\n",
    "        # Middle (bottom)\n",
    "        x = self.middle(x, t)\n",
    "\n",
    "        # Second half of U-Net\n",
    "        for m in self.up:\n",
    "            if isinstance(m, Upsample):\n",
    "                x = m(x, t)\n",
    "            else:\n",
    "                # Get the skip connection from first half of U-Net and concatenate\n",
    "                s = h.pop()\n",
    "                x = torch.cat((x, s), dim=1)\n",
    "                #\n",
    "                x = m(x, t)\n",
    "\n",
    "        # Final normalization and convolution\n",
    "        return self.final(self.act(self.norm(x)))\n",
    "    \n",
    "    \n",
    "#@title DISC\n",
    "class DISC(nn.Module):\n",
    "    def __init__(self, image_channels: int = 3, n_channels: int = 64,\n",
    "                 ch_mults: Union[Tuple[int, ...], List[int]] = (1, 2, 2, 4),\n",
    "                 is_attn: Union[Tuple[bool, ...], List[int]] = (False, False, True, True),\n",
    "                 n_blocks: int = 2):\n",
    "        \"\"\"\n",
    "        * `image_channels` is the number of channels in the image. $3$ for RGB.\n",
    "        * `n_channels` is number of channels in the initial feature map that we transform the image into\n",
    "        * `ch_mults` is the list of channel numbers at each resolution. The number of channels is `ch_mults[i] * n_channels`\n",
    "        * `is_attn` is a list of booleans that indicate whether to use attention at each resolution\n",
    "        * `n_blocks` is the number of `UpDownBlocks` at each resolution\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Number of resolutions\n",
    "        n_resolutions = len(ch_mults)\n",
    "\n",
    "        # Project image into feature map\n",
    "        self.image_proj = nn.Conv2d(image_channels, n_channels, kernel_size=(3, 3), padding=(1, 1))\n",
    "\n",
    "        # TODO n_time_channels, n_cloob_channels\n",
    "        # TODOc should be optional maybe?\n",
    "\n",
    "        # Time embedding layer. Time embedding has `n_channels * 4` channels in the original, here 2 (to make room for cloob\n",
    "        n_time_channels = n_channels * 4\n",
    "        self.time_emb = TimeEmbedding(n_time_channels)\n",
    "        \n",
    "        # CLOOB embeddings\n",
    "        n_cloob_channels  = n_channels * 4\n",
    "        self.cloob_emb = CLOOBEmbedding(n_cloob_channels)\n",
    "        \n",
    "        n_cond_channels = n_time_channels+n_cloob_channels\n",
    "        \n",
    "        # #### First half of U-Net - decreasing resolution\n",
    "        down = []\n",
    "        # Number of channels\n",
    "        out_channels = in_channels = n_channels\n",
    "        # For each resolution\n",
    "        for i in range(n_resolutions):\n",
    "            # Number of output channels at this resolution\n",
    "            out_channels = in_channels * ch_mults[i]\n",
    "            # Add `n_blocks`\n",
    "            for _ in range(n_blocks):\n",
    "                down.append(DownBlock(in_channels, out_channels, n_cond_channels, is_attn[i]))\n",
    "                in_channels = out_channels\n",
    "            # Down sample at all resolutions except the last\n",
    "            if i < n_resolutions - 1:\n",
    "                down.append(Downsample(in_channels))\n",
    "\n",
    "        # Combine the set of modules\n",
    "        self.down = nn.ModuleList(down)\n",
    "\n",
    "        # Middle block\n",
    "        self.middle = MiddleBlock(out_channels, n_cond_channels, )\n",
    "\n",
    "        # Final 'head'\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(out_channels, 1), # TODO add a second MLP layer here maybe?\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor, c:torch.Tensor):\n",
    "        \"\"\"\n",
    "        * `x` has shape `[batch_size, in_channels, height, width]`\n",
    "        * `t` has shape `[batch_size]`\n",
    "        \"\"\"\n",
    "\n",
    "        # Get time-step embeddings\n",
    "        t = self.time_emb(t)\n",
    "\n",
    "        # TODO handle when c and or z are None\n",
    "        # TODO both c and z should be optional\n",
    "        \n",
    "        # Combine with the cloob & z embeddings (hackity hack)\n",
    "        c = self.cloob_emb(c)\n",
    "        t = torch.cat((t, c), dim=1)\n",
    "\n",
    "        # Get image projection\n",
    "        x = self.image_proj(x)\n",
    "\n",
    "        # `h` will store outputs at each resolution for skip connection\n",
    "        h = [x]\n",
    "        # First half of U-Net\n",
    "        for m in self.down:\n",
    "            x = m(x, t)\n",
    "            h.append(x)\n",
    "\n",
    "        # Middle (bottom)\n",
    "        x = self.middle(x, t)\n",
    "\n",
    "        return self.head(x) # Final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 16, 16]), torch.Size([1, 4, 16, 16]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "unet = UNet(image_channels=4).to(device)\n",
    "z = torch.randn((1,8), device=device)\n",
    "c = torch.zeros((1,512), device=device)\n",
    "x = torch.randn(1, 4, 16, 16).to(device)\n",
    "t = torch.tensor(3, dtype=torch.long).unsqueeze(0).to(device)\n",
    "pred_im = unet(x.float(), t, c, z)\n",
    "x.shape, pred_im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
